import numpy as np
from mytorch import tensor
from mytorch.tensor import Tensor
from mytorch.nn.module import Module
from mytorch.nn.activations import Tanh, ReLU, Sigmoid
from mytorch.nn.util import PackedSequence, pack_sequence, unpack_sequence
from mytorch.nn.rnn import TimeIterator


class GRUUnit(Module):
    '''
    This class defines a single GRU Unit block.
    
    NOTE: *args is placed just to ignore the nonlinearity parameter that it recevies from GRU module as GRUs have fixed set of activation functions that are called unlike RNNs. Given we will be using the same rnn.TimeIterator class to construct GRU we need to ignore the nonlinearity parameter. 

    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the GRUUnit at each timestep
    '''
    def __init__(self, input_size, hidden_size, *args, **kwargs ):
        super(GRUUnit,self).__init__()
        
        # Initializing parameters
        self.weight_ir = Tensor(np.random.randn(hidden_size,input_size), requires_grad=True, is_parameter=True)
        self.bias_ir   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.weight_iz = Tensor(np.random.randn(hidden_size,input_size), requires_grad=True, is_parameter=True)
        self.bias_iz   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.weight_in = Tensor(np.random.randn(hidden_size,input_size), requires_grad=True, is_parameter=True)
        self.bias_in   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.weight_hr = Tensor(np.random.randn(hidden_size,hidden_size), requires_grad=True, is_parameter=True)
        self.bias_hr   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.weight_hz = Tensor(np.random.randn(hidden_size,hidden_size), requires_grad=True, is_parameter=True)
        self.bias_hz   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.weight_hn = Tensor(np.random.randn(hidden_size,hidden_size), requires_grad=True, is_parameter=True)
        self.bias_hn   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        
        self.hidden_size = hidden_size

    
    def __call__(self, input, hidden = None):
        return self.forward(input,hidden)

    def forward(self, input, hidden = None):
        '''
        Args:
            input (Tensor): (effective_batch_size,input_size)
            hidden (Tensor,None): (effective_batch_size,hidden_size)
        Return:
            Tensor: (effective_batch_size,hidden_size)
        '''

        # TODO: INSTRUCTIONS
        # Perform matrix operations to construct the intermediary value from input and hidden tensors
        # Remeber to handle the case when hidden = None. Construct a tensor of appropriate size, filled with 0s to use as the hidden.
        batch_size,_ = input.shape
        if hidden is None:
            hidden = tensor.Tensor(np.zeros((batch_size, self.hidden_size)), 
                                   requires_grad=True, is_parameter=True)
        X, H = input, hidden
        W_xz, W_hz, b_iz, b_hz= self.weight_iz, self.weight_hz,  self.bias_iz ,self.bias_hz 
        W_xr, W_hr, b_ir, b_hr = self.weight_ir, self.weight_hr, self.bias_ir, self.bias_hr
        W_xh, W_hh, b_ih, b_hh = self.weight_in, self.weight_hn, self.bias_in, self.bias_hn
        act_sig = Sigmoid()
        act_tanh = Tanh()
        Z = act_sig((X @ W_xz.T()) + b_iz + (H @ W_hz.T()) + b_hz)
        R = act_sig((X @ W_xr.T()) + b_ir + (H @ W_hr.T()) + b_hr)
        H_tilda = act_tanh((X @ W_xh.T()) + b_ih + R * (H @ W_hh.T() + b_hh))
        return  Z * H + (tensor.Tensor.ones(*Z.shape) - Z) * H_tilda
       
class GRU(TimeIterator):
    '''
    Child class for TimeIterator which appropriately initializes the parent class to construct an GRU.
    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the GRUUnit at each timestep
    '''

    def __init__(self, input_size, hidden_size ):
        super().__init__(GRUUnit, input_size, hidden_size)


